# -*- coding: utf-8 -*-
"""002_langchain_RAG.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1bHtyHKFWFgaSB2kpHSCrchVi1SXB9LS4

# Part2: RAG(Retrieval-Augmented Generation)
- 모델의 학습 데이터에 포함되지 않은 데이터를 사용 (환각 방지)
- 외부 데이터를 검색(retrieval)한 후, 생성(generation) 단계에서 LLM에 전달
- 모델은 주어진 컨텍스트나 질문에 더 적합하고 풍부한 정보를 반영한 답변을 생성

## 0. 환경 구성
"""

# !pip install -q langchain langchain-openai langchain_community tiktoken chromadb

import langchain

langchain.__version__

from dotenv import load_dotenv

load_dotenv()

os.environ['OPENAI_API_KEY'] = os.getenv('SECRET_KEY')

"""## 1. RAG 파이프라인 개요


### Step 1: 데이터 불러오기
- 웹페이지에서 데이터 가져오기


"""

# Data Loader - 웹페이지 데이터 가져오기
from langchain_community.document_loaders import WebBaseLoader

# 위키피디아 정책과 지침
url = 'https://ko.wikipedia.org/wiki/%EC%9C%84%ED%82%A4%EB%B0%B1%EA%B3%BC:%EC%A0%95%EC%B1%85%EA%B3%BC_%EC%A7%80%EC%B9%A8'
loader = WebBaseLoader(url)

# 웹페이지 텍스트 -> Documents
docs = loader.load()

print(len(docs))
print(len(docs[0].page_content))
print(docs[0].page_content[5000:6000])

"""### Step 2: Split texts"""

# Text Split (Documents -> small chunks: Documents)
from langchain.text_splitter import RecursiveCharacterTextSplitter

# 덩어리 사이즈 지정, 겹치는 영역 사이즈 지정
text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)

# 문장의 끝까지 나눠줌. 문장이 중간에 끊겨 불러와지지 않도록 함
splits = text_splitter.split_documents(docs)

print(len(splits))
print(splits[10])

# page_content 속성
splits[10].page_content

# metadata 속성
splits[10].metadata

"""### Step 3: Indexing
벡터 DB에 Split된 text를 인덱싱해줌
"""

# Indexing (Texts -> Embedding -> Store)
from langchain_community.vectorstores import Chroma
from langchain_openai import OpenAIEmbeddings

# 벡터 db 생성
vectorstore = Chroma.from_documents(documents=splits,
                                    embedding=OpenAIEmbeddings())

# 벡터 db에서 특정 내용과 관련된 청크 가져오기
docs = vectorstore.similarity_search("격하 과정에 대해서 설명해주세요.")
print(len(docs))
print(docs[0].page_content)

"""### Step 4: Retrieval ~ Generation
벡터 DB에서 유사한 내용을 불러와 prompt에 포함시키기
"""

from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser

# Prompt: 내가 첨부한 내용에 근거하여 답변하도록 프롬프팅
template = '''Answer the question based only on the following context:
{context}

Question: {question}
'''

prompt = ChatPromptTemplate.from_template(template)

# LLM (주의: temparature을 낮춰서 환각현상을 최소화하도록 하기)
model = ChatOpenAI(model='gpt-3.5-turbo-0125', temperature=0)

# Rretriever
retriever = vectorstore.as_retriever()

# Combine Documents
def format_docs(docs):
    return '\n\n'.join(doc.page_content for doc in docs)

# RAG Chain 연결
rag_chain = (
    {'context': retriever | format_docs, 'question': RunnablePassthrough()}
    | prompt
    | model
    | StrOutputParser()
)

# Chain 실행
rag_chain.invoke("격하 과정에 대해서 설명해주세요.")

