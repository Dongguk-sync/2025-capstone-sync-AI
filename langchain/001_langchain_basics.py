# -*- coding: utf-8 -*-
"""001_langchain_basics.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1uCiFzyc5MP9dpjnJ9v7Myj7OINYtmoYp

# Part1: Langchain 기초
## 1. 환경 구성
### 1) 라이브러리 설치
"""

!pip install -q langchain langchain-openai tiktoken
!pip install --upgrade langchain langchain-core langchain-openai

"""### 2) OpenAI 인증키 설정"""

import os
os.environ['OPENAI_API_KEY'] = 'OPENAI_API_KEY'

from langchain_openai import ChatOpenAI

# model
llm = ChatOpenAI(model='gpt-4.1-nano')

# chain 실행
result = llm.invoke("지구의 자전 주기는?")

result

result.content

"""
## 2. LLM Chain
## 1) Prompt + LLM"""

from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser

# prompt + model + output parser
prompt = ChatPromptTemplate.from_template("You are an expert in astronomy. Answer the question. <Question>: {input}")
llm = ChatOpenAI(model='gpt-4.1-nano')
output_parsers = StrOutputParser()

# LCEL chaining
chain = prompt | llm | output_parsers

# chain 호출
chain.invoke({"input":"지구의 자전 주기는?"})

"""### 2) Multiple Chains"""

prompt1 = ChatPromptTemplate.from_template("translates {korean_word} to English. Answer only one word.")
prompt2 = ChatPromptTemplate.from_template(
    "explain {english_word} using oxford dictionary to me in Korean."
)

llm = ChatOpenAI(model='gpt-4.1-nano')

chain1 = prompt1 | llm | StrOutputParser()

chain1.invoke({"korean_word":"미래"})

chain2 = (
    {"english_word":chain1}
    | prompt2
    | llm
    | StrOutputParser()
)

chain2.invoke({"korean_word":"미래"})

"""## 3. Prompt
### 1) PromptTemplate
"""

from langchain_core.prompts import PromptTemplate

# 'name'과 'age'라는 두 개의 변수를 사용하는 프롬프트 템플릿을 정의
template_text = "안녕하세요, 제 이름은 {name}이고, 나이는 {age}살입니다."

# PromptTemplate 인스턴스를 생성
prompt_template = PromptTemplate.from_template(template_text)

# 템플릿에 값을 채워서 프롬프트를 완성
filled_prompt = prompt_template.format(name="홍길동", age=30)

filled_prompt

# 문자열 템플릿 결합 (PromptTemplate + PromptTemplate + 문자열)
combined_prompt = (
              prompt_template
              + PromptTemplate.from_template("\n\n아버지를 아버지라 부를 수 없습니다.")
              + "\n\n{language}로 번역해주세요."
)

combined_prompt

combined_prompt.format(name="홍길동", age=30, language="영어")

from langchain_openai import ChatOpenAI
from langchain_core.output_parsers import StrOutputParser

llm = ChatOpenAI(model="gpt-3.5-turbo-0125")
chain = combined_prompt | llm | StrOutputParser()
chain.invoke({"age":30, "language":"영어", "name":"홍길동"})

"""### 2) ChatPromptTemplate
- ChatPromptTemplate + ChatModels
- 여러 메세지 입력 -> 단일 메세지 출력
- 채팅 메세지를 원소로 갖는 리스트 형태
- 구성: 각 채팅 메세지는 role과 content가 짝을 이룸
"""

# 2-튜플 형태의 메시지 목록으로 프롬프트 생성 (type, content)

from langchain_core.prompts import ChatPromptTemplate

chat_prompt = ChatPromptTemplate.from_messages([
    ("system", "이 시스템은 천문학 질문에 답변할 수 있습니다."),
    ("user", "{user_input}"),
])

messages = chat_prompt.format_messages(user_input="태양계에서 가장 큰 행성은 무엇인가요?")
messages

chain = chat_prompt | llm | StrOutputParser()

chain.invoke({"user_input": "태양계에서 가장 큰 행성은 무엇인가요?"})

"""### 3) Message"""

# MessagePromptTemplate 활용

from langchain_core.prompts import SystemMessagePromptTemplate,  HumanMessagePromptTemplate

chat_prompt = ChatPromptTemplate.from_messages(
    [
        SystemMessagePromptTemplate.from_template("이 시스템은 천문학 질문에 답변할 수 있습니다."),
        HumanMessagePromptTemplate.from_template("{user_input}"),
    ]
)


messages = chat_prompt.format_messages(user_input="태양계에서 가장 큰 행성은 무엇인가요?")
messages

chain = chat_prompt | llm | StrOutputParser()

chain.invoke({"user_input": "태양계에서 가장 큰 행성은 무엇인가요?"})

"""## 5. Model Parameter
### 1) 모델 클래스 유형
  LLM
"""

from langchain_openai import OpenAI

llm = OpenAI()

llm.invoke("한국의 대표적인 관광지 3군데를 추천해주세요.")

from langchain_core.prompts import ChatPromptTemplate
from langchain_openai import ChatOpenAI

chat = ChatOpenAI()

chat_prompt = ChatPromptTemplate.from_messages([
    ("system", "이 시스템은 여행 전문가입니다."),
    ("user", "{user_input}"),
])

chain = chat_prompt | chat
chain.invoke({"user_input": "안녕하세요? 한국의 대표적인 관광지 3군데를 추천해주세요."})

"""위에서 다음 세가지 메세지를 볼 수 있음
- system message
- user message
- AI message (응답 메세지)

### 2) 모델 파라미터 설정
- **Temperature**: 생성된 텍스트의 다양성을 조정합니다. 값이 작으면 예측 가능하고 일관된 출력을 생성하는 반면, 값이 크면 다양하고 예측하기 어려운 출력을 생성합니다.
- **Max Tokens** (최대 토큰 수): 생성할 최대 토큰 수를 지정합니다. 생성할 텍스트의 길이를 제한합니다.
- **Top P** (Top Probability): 생성 과정에서 특정 확률 분포 내에서 상위 P% 토큰만을 고려하는 방식입니다. 이는 출력의 다양성을 조정하는 데 도움이 됩니다.
- **Frequency Penalty** (빈도 패널티): 값이 클수록 이미 등장한 단어나 구절이 다시 등장할 확률을 감소시킵니다. 이를 통해 반복을 줄이고 텍스트의 다양성을 증가시킬 수 있습니다. (0~1)
- **Presence Penalty** (존재 패널티): 텍스트 내에서 단어의 존재 유무에 따라 그 단어의 선택 확률을 조정합니다. 값이 클수록 아직 텍스트에 등장하지 않은 새로운 단어의 사용이 장려됩니다. (0~1)
- **Stop Sequences** (정지 시퀀스): 특정 단어나 구절이 등장할 경우 생성을 멈추도록 설정합니다. 이는 출력을 특정 포인트에서 종료하고자 할 때 사용됩니다.


Temparature, Top p, Frequency Penalty, Presence Panalty는 모두 유사한 기능을 함.

### 모델에 직접 파라미터를 전달 (at **모델 생성 시점**)
"""

from langchain_openai import ChatOpenAI

# 모델 파라미터 설정
params = {
    "temperature": 0.7,         # 생성된 텍스트의 다양성 조정
    "max_tokens": 100,          # 생성할 최대 토큰 수
}

kwargs = {
    "frequency_penalty": 0.5,   # 이미 등장한 단어의 재등장 확률
    "presence_penalty": 0.5,    # 새로운 단어의 도입을 장려
    "stop": ["\n"]              # 정지 시퀀스 설정

}

# 모델 인스턴스를 생성할 때 설정
model = ChatOpenAI(model="gpt-3.5-turbo-0125", **params, model_kwargs = kwargs)


# 모델 호출
question = "태양계에서 가장 큰 행성은 무엇인가요?"
response = model.invoke(input=question)

# 전체 응답 출력
print(response)

"""	•	params → 딕셔너리 자체
	•	**params → 딕셔너리를 풀어서 key=value 형태로 전달
	•	이는 함수에 인자를 동적으로 전달할 때 필수적인 파이썬 문법입니다.
"""

# 모델 파라미터 설정
params = {
    "temperature": 0.7,         # 생성된 텍스트의 다양성 조정
    "max_tokens": 10,          # 생성할 최대 토큰 수
}

# 모델 인스턴스를 호출할 때 전달
response = model.invoke(input=question, **params)

# 문자열 출력
print(response.content)

"""### 모델에 추가적인 파라미터를 전달
**- bind 메서드 사용**
"""

from langchain_core.prompts import ChatPromptTemplate

prompt = ChatPromptTemplate.from_messages([
    ("system", "이 시스템은 천문학 질문에 답변할 수 있습니다."),
    ("user", "{user_input}"),
])

model = ChatOpenAI(model="gpt-3.5-turbo-0125", max_tokens=100)

messages = prompt.format_messages(user_input="태양계에서 가장 큰 행성은 무엇인가요?")

before_answer = model.invoke(messages)

# # binding 이전 출력
print(before_answer)

# 모델 호출 시 추가적인 인수를 전달하기 위해 bind 메서드 사용 (응답의 최대 길이를 10 토큰으로 제한)
# max_tokens의 변동 사항이 있음!
chain = prompt | model.bind(max_tokens=10)

after_answer = chain.invoke({"user_input": "태양계에서 가장 큰 행성은 무엇인가요?"})

# binding 이후 출력
print(after_answer)

"""## 6. Output Parsers
### 1) CSV Parser
"""

