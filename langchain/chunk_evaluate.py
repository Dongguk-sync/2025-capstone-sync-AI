import os

from langchain_teddynote import logging

logging.langsmith("Beakji-evaluate")

from dotenv import load_dotenv
from langchain_chroma import Chroma

load_dotenv()
persist_directory = os.getenv("PERSIST_DIRECTORY")
os.environ["OPENAI_API_KEY"] = os.getenv("OPENAI_API_KEY")

from langchain_community.vectorstores import Chroma
from langchain_openai import OpenAIEmbeddings
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser
from split import join_docs
from signup import get_or_create_user_chromadb

SIMILARITY_THRESHOLD = 0.2


def get_evaluation_prompt():
    return """Evaluate <student answer> based on the <answer key>.

    - Feedback is based on the answer key.
    - Don't evaluate information that's not in the answer key.
    - Focus on conceptual accuracy rather than wording
    - Recognize that foreign proper nouns may be spelled differently.
    - Please write in JSON format and Korean.
    - Organize your feedback under these 2 headings: 
        - ## ëˆ„ë½ëœ ë‚´ìš© (Missing)
        - ## í‹€ë¦° ë‚´ìš© (Incorrect)

    <Answer key>:
    {answer_key}

    <Student answer>:
    {student_answer}

    <return example>:
    {{
        "missing": [...],
        "incorrect": [...],
    }}
    """


def chunk_evaluate(vectorstore: Chroma, subject: str, unit: str, answer_key_chunk: str):

    # ë²¡í„° dbì—ì„œ ì •ë‹µê³¼ ê´€ë ¨ëœ ë‹µì•ˆ ì²­í¬ ê°€ì ¸ì˜¤ê¸°
    similar_chunks = vectorstore.similarity_search_with_score(
        answer_key_chunk,  # ì •ë‹µ ì²­í¬
        k=5,  # ìœ ì‚¬ë„ ìƒìœ„ 5ê°œ ì²­í¬ë§Œ ê²€ìƒ‰
        filter={
            "$and": [
                {"subject": {"$eq": subject}},
                {"unit": {"$eq": unit}},
                {"type": {"$eq": "student_answer"}},
            ]
        },
    )

    # ìœ ì‚¬ë„ ì ìˆ˜ê°€ SIMILARITY_THRESHOLD ì´í•˜(ìœ ì‚¬ë„ ë†’ìŒ)ì¸ ê²ƒë§Œ í•„í„°ë§
    filtered_chunks = [
        doc for doc, score in similar_chunks if score <= SIMILARITY_THRESHOLD
    ]

    if not filtered_chunks:
        print("â— ìœ ì‚¬í•œ í•™ìƒ ë‹µë³€ ì—†ìŒ (ì „ì²´ í•­ëª© ëˆ„ë½)")
        return None

    # ì¶”ì¶œí•œ ì²­í¬ í†µí•©
    all_student_answer_chunk = join_docs(filtered_chunks)

    template = get_evaluation_prompt()

    prompt = ChatPromptTemplate.from_template(template)
    model = ChatOpenAI(model="gpt-3.5-turbo", temperature=0)
    rag_chain = RunnablePassthrough() | prompt | model | StrOutputParser()

    try:
        result = rag_chain.invoke(
            {"answer_key": answer_key_chunk, "student_answer": all_student_answer_chunk}
        )
    except Exception as e:
        logging.error("ğŸ›‘ í‰ê°€ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: %s", e)
        return None

    # feedbackì„ ChromaDBì— ì €ì¥
    vectorstore.add_texts(
        texts=[result],
        metadatas=[
            {
                "subject": subject,
                "unit": unit,
                "type": "feedback",
            }
        ],
    )
    return result


if __name__ == "__main__":
    user_id = "user123"
    subject = "ì§€êµ¬ê³¼í•™"
    unit = "íŒêµ¬ì¡°ë¡  ì •ë¦½ ê³¼ì •"
    index = 0
    answer_key_id = f"{subject}_{unit}_answer_key_{index}"

    vectorstore = get_or_create_user_chromadb(user_id)
    retrieved = vectorstore.get(ids=[answer_key_id])

    if not retrieved["documents"]:
        print(f"âŒ ID '{answer_key_id}'ì— í•´ë‹¹í•˜ëŠ” answer keyë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        exit(1)

    answer_key_chunk = retrieved["documents"][0]
    result = chunk_evaluate(vectorstore, subject, unit, answer_key_chunk)
