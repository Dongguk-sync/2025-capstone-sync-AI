import os

from langchain_teddynote import logging

logging.langsmith("Beakji-evaluate")

from dotenv import load_dotenv

load_dotenv()
persist_directory = os.getenv("PERSIST_DIRECTORY")
os.environ["OPENAI_API_KEY"] = os.getenv("OPENAI_API_KEY")

from langchain_community.vectorstores import Chroma
from langchain_openai import OpenAIEmbeddings
from split import join_docs
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser

SIMILARITY_THRESHOLD = 0.25


def get_evaluation_prompt():
    return """Evaluate <student answer> based on the <answer key>.

    - Feedback is based on the answer key.
    - Don't evaluate information that's not in the answer key.
    - Focus on conceptual accuracy rather than wording
    - Recognize that foreign proper nouns may be spelled differently.
    - Please write in markdown format and Korean.
    - Organize your feedback under these 2 headings: 
        - ## ëˆ„ë½ëœ ë‚´ìš© (Missing)
        - ## í‹€ë¦° ë‚´ìš© (Incorrect)

    <Answer key>:
    {answer_key}

    <Student answer>:
    {student_answer}
    """


def chunk_evaluate(vectorstore: Chroma, subject: str, unit: str, answer_key_chunk: str):

    # ë²¡í„° dbì—ì„œ ì •ë‹µê³¼ ê´€ë ¨ëœ ë‹µì•ˆ ì²­í¬ ê°€ì ¸ì˜¤ê¸°
    similar_chunks = vectorstore.similarity_search_with_score(
        answer_key_chunk,  # ì •ë‹µ ì²­í¬
        k=3,  # ìœ ì‚¬ë„ ìƒìœ„ 3ê°œ ì²­í¬ë§Œ ê²€ìƒ‰
        filter={
            "$and": [
                {"subject": {"$eq": subject}},
                {"unit": {"$eq": unit}},
                {"type": {"$eq": "student_answer"}},
            ]
        },
    )

    # ìœ ì‚¬ë„ ì ìˆ˜ê°€ 0.25 ì´í•˜(ìœ ì‚¬ë„ ë†’ìŒ)ì¸ ê²ƒë§Œ í•„í„°ë§
    filtered_chunks = [
        doc for doc, score in similar_chunks if score <= SIMILARITY_THRESHOLD
    ]

    if not filtered_chunks:
        logging.warning("â— ìœ ì‚¬í•œ í•™ìƒ ë‹µë³€ ì—†ìŒ")
        return None

    # ì¶”ì¶œí•œ ì²­í¬ í†µí•©
    all_student_answer_chunk = join_docs(docs=filtered_chunks)

    template = get_evaluation_prompt()

    prompt = ChatPromptTemplate.from_template(template)
    model = ChatOpenAI(model="gpt-3.5-turbo", temperature=0)
    rag_chain = RunnablePassthrough() | prompt | model | StrOutputParser()

    try:
        result = rag_chain.invoke(
            {"answer_key": answer_key_chunk, "student_answer": all_student_answer_chunk}
        )
    except Exception as e:
        logging.error("ğŸ›‘ í‰ê°€ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: %s", e)
        return None

    # feedbackì„ ChromaDBì— ì €ì¥
    vectorstore.add_texts(
        texts=[result],
        metadatas=[
            {
                "subject": subject,
                "unit": unit,
                "type": "feedback",
            }
        ],
    )

    return result
